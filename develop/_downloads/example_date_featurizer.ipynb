{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Modeling quasi-seasonal trends with date features\n\n\n\nSome trends are common enough to appear seasonal, yet sporadic enough that\napproaching them from a seasonal perspective may not be valid. An example of\nthis is the `\"end-of-the-month\" effect <https://robjhyndman.com/hyndsight/monthly-seasonality/>`_.\nIn this example, we'll explore how we can create meaningful features that\nexpress seasonal trends without needing to fit a seasonal model.\n\n.. raw:: html\n\n   <br/>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\n# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\n\nimport pmdarima as pm\nfrom pmdarima import arima\nfrom pmdarima import model_selection\nfrom pmdarima import pipeline\nfrom pmdarima import preprocessing\nfrom pmdarima.datasets._base import load_date_example\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nprint(\"pmdarima version: %s\" % pm.__version__)\n\n# Load the data and split it into separate pieces\ny, X = load_date_example()\ny_train, y_test, X_train, X_test = \\\n    model_selection.train_test_split(y, X, test_size=20)\n\n# We can examine traits about the time series:\npm.tsdisplay(y_train, lag_max=10)\n\n# We can see the ACF increases and decreases rather rapidly, which means we may\n# need some differencing. There also does not appear to be an obvious seasonal\n# trend.\nn_diffs = arima.ndiffs(y_train, max_d=5)\n\n# Here's what the featurizer will create for us:\ndate_feat = preprocessing.DateFeaturizer(\n    column_name=\"date\",  # the name of the date feature in the exog matrix\n    with_day_of_week=True,\n    with_day_of_month=True)\n\n_, X_train_feats = date_feat.fit_transform(y_train, X_train)\nprint(\"Head of generated exog features:\\n%s\" % repr(X_train_feats.head()))\n\n# We can plug this exog featurizer into a pipeline:\npipe = pipeline.Pipeline([\n    ('date', date_feat),\n    ('arima', arima.AutoARIMA(d=n_diffs,\n                              trace=3,\n                              stepwise=True,\n                              suppress_warnings=True,\n                              seasonal=False))\n])\n\npipe.fit(y_train, X_train)\n\n# Plot our forecasts\nforecasts = pipe.predict(exogenous=X_test)\n\nfig = plt.figure(figsize=(16, 8))\nax = fig.add_subplot(1, 1, 1)\n\nn_train = y_train.shape[0]\nx = np.arange(n_train + forecasts.shape[0])\n\nax.plot(x[:n_train], y_train, color='blue', label='Training Data')\nax.plot(x[n_train:], forecasts, color='green', marker='o',\n        label='Predicted')\nax.plot(x[n_train:], y_test, color='red', label='Actual')\nax.legend(loc='lower left', borderaxespad=0.5)\nax.set_title('Predicted Foo')\nax.set_ylabel('# Foo')\n\nplt.show()\n\n# What next? Try combining different featurizers in your pipeline to enhance\n# a model's predictive power."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}